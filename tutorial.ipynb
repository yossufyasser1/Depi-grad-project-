{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9da23f6f",
   "metadata": {},
   "source": [
    "# üéì AI Study Assistant - Complete Tutorial\n",
    "\n",
    "Welcome to the comprehensive tutorial for building and using the AI Study Assistant!\n",
    "\n",
    "This notebook will guide you through:\n",
    "- ‚úÖ Environment setup and dependencies\n",
    "- üê≥ Docker configuration\n",
    "- üìö Document processing and RAG pipeline\n",
    "- ü§ñ Model loading and inference\n",
    "- üöÄ FastAPI deployment\n",
    "- ‚ö° Performance optimization\n",
    "- üß™ Testing and benchmarking\n",
    "\n",
    "**Estimated Time:** 2-3 hours\n",
    "\n",
    "**Prerequisites:**\n",
    "- Python 3.11+\n",
    "- Docker (optional)\n",
    "- Basic knowledge of NLP and REST APIs\n",
    "\n",
    "Let's get started! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb5ce45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b187d58d",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Environment Setup and Dependencies\n",
    "\n",
    "First, let's install all required libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49890f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we're in the project root directory\n",
    "import os\n",
    "import sys\n",
    "\n",
    "project_root = os.path.abspath('.')\n",
    "print(f\"Project Root: {project_root}\")\n",
    "\n",
    "# Add project to Python path\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "    \n",
    "print(\"‚úì Environment configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35f37bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core libraries\n",
    "import torch\n",
    "import transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "import requests\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "import time\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(\"‚úì Core libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90120ef9",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Configuration and Setup\n",
    "\n",
    "Load configuration from the project's Config class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9bd09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "from src.config import Config\n",
    "\n",
    "config = Config()\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"  - Embedding Model: {config.EMBEDDING_MODEL_NAME}\")\n",
    "print(f\"  - LLM Model: {config.LLM_MODEL_NAME}\")\n",
    "print(f\"  - Summarizer Model: {config.SUMMARIZER_MODEL}\")\n",
    "print(f\"  - Vector DB Path: {config.VECTOR_DB_PATH}\")\n",
    "print(f\"  - Batch Size: {config.BATCH_SIZE}\")\n",
    "print(f\"  - Max Length: {config.MAX_LENGTH}\")\n",
    "print(\"‚úì Configuration ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfc3fd2",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Text Preprocessing\n",
    "\n",
    "Let's test the text preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bcec9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test text preprocessing\n",
    "from src.preprocessing.text_preprocessor import TextPreprocessor\n",
    "\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "sample_text = \"\"\"\n",
    "Machine learning is a subset of artificial intelligence. \n",
    "It enables computers to learn from data without being explicitly programmed.\n",
    "Deep learning uses neural networks with multiple layers.\n",
    "\"\"\"\n",
    "\n",
    "result = preprocessor.preprocess_full(sample_text)\n",
    "\n",
    "print(\"Preprocessing Results:\")\n",
    "print(f\"  - Sentences: {len(result['sentences'])}\")\n",
    "print(f\"  - Tokens: {len(result['tokens'])}\")\n",
    "print(f\"  - Lemmatized: {' '.join(result['lemmatized'][:10])}...\")\n",
    "print(f\"  - POS Tags: {result['pos_tags'][:5]}\")\n",
    "print(\"‚úì Text preprocessing works\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c833e073",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Keyword Extraction\n",
    "\n",
    "Test the RAKE-based keyword extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ede7d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test keyword extraction\n",
    "from src.training.keyword_extractor import KeywordExtractor\n",
    "\n",
    "extractor = KeywordExtractor()\n",
    "\n",
    "text = \"\"\"\n",
    "Python programming language is widely used in data science and machine learning.\n",
    "Natural language processing and computer vision are key areas of artificial intelligence.\n",
    "Deep learning models require significant computational resources and large datasets.\n",
    "\"\"\"\n",
    "\n",
    "keywords = extractor.extract_keywords(text, top_n=10)\n",
    "\n",
    "print(\"Top Keywords:\")\n",
    "for keyword, score in keywords:\n",
    "    print(f\"  - {keyword}: {score:.3f}\")\n",
    "print(\"‚úì Keyword extraction works\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a69163c",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ TextRank Summarization\n",
    "\n",
    "Test extractive summarization using TextRank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6daa5dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test TextRank summarization\n",
    "from src.training.textrank_summarizer import TextRankSummarizer\n",
    "\n",
    "summarizer = TextRankSummarizer(num_sentences=2)\n",
    "\n",
    "long_text = \"\"\"\n",
    "Machine learning is a method of data analysis that automates analytical model building.\n",
    "It is a branch of artificial intelligence based on the idea that systems can learn from data.\n",
    "Machine learning algorithms build a model based on sample data, known as training data.\n",
    "The algorithms make predictions or decisions without being explicitly programmed to do so.\n",
    "Machine learning algorithms are used in a wide variety of applications.\n",
    "Email filtering and computer vision are examples where it is difficult to develop conventional algorithms.\n",
    "Machine learning is closely related to computational statistics and mathematical optimization.\n",
    "\"\"\"\n",
    "\n",
    "summary = summarizer.summarize(long_text)\n",
    "\n",
    "print(\"Original Length:\", len(long_text))\n",
    "print(\"Summary Length:\", len(summary))\n",
    "print(f\"Compression: {len(summary)/len(long_text)*100:.1f}%\")\n",
    "print(\"\\nSummary:\")\n",
    "print(summary)\n",
    "print(\"‚úì TextRank summarization works\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beae5484",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Vector Database with ChromaDB\n",
    "\n",
    "Initialize and test ChromaDB for semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5a3452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ChromaDB\n",
    "from src.inference.chromadb_manager import ChromaDBManager\n",
    "\n",
    "# Use a test database\n",
    "db_manager = ChromaDBManager(persist_directory=\"./test_chroma_db\")\n",
    "\n",
    "# Add sample documents\n",
    "documents = [\n",
    "    \"Python is a high-level programming language.\",\n",
    "    \"Machine learning is a subset of artificial intelligence.\",\n",
    "    \"Neural networks are inspired by biological brains.\",\n",
    "    \"Deep learning uses multiple layers in neural networks.\",\n",
    "    \"Natural language processing deals with text and speech.\"\n",
    "]\n",
    "\n",
    "metadata = [\n",
    "    {\"source\": \"python_doc\", \"type\": \"definition\"},\n",
    "    {\"source\": \"ml_doc\", \"type\": \"definition\"},\n",
    "    {\"source\": \"nn_doc\", \"type\": \"explanation\"},\n",
    "    {\"source\": \"dl_doc\", \"type\": \"explanation\"},\n",
    "    {\"source\": \"nlp_doc\", \"type\": \"definition\"}\n",
    "]\n",
    "\n",
    "ids = [f\"doc_{i}\" for i in range(len(documents))]\n",
    "\n",
    "db_manager.add_documents(documents, metadata, ids)\n",
    "\n",
    "# Check collection stats\n",
    "stats = db_manager.get_collection_stats()\n",
    "print(f\"Collection Stats: {stats}\")\n",
    "print(\"‚úì ChromaDB initialized with documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58451bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test semantic search\n",
    "query = \"What is machine learning?\"\n",
    "results = db_manager.query(query, n_results=3)\n",
    "\n",
    "print(f\"\\nQuery: {query}\")\n",
    "print(f\"\\nTop {len(results)} Results:\")\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"\\n{i}. Score: {result['score']:.3f}\")\n",
    "    print(f\"   Text: {result['text']}\")\n",
    "    print(f\"   Metadata: {result['metadata']}\")\n",
    "print(\"‚úì Semantic search works\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c57c36",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ RAG Pipeline\n",
    "\n",
    "Build and test the complete RAG (Retrieval-Augmented Generation) pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d2e621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RAG components\n",
    "from src.inference.rag_retriever import RAGRetriever\n",
    "from src.inference.llm_reader import LLMReader\n",
    "\n",
    "# Create retriever with our vector store\n",
    "retriever = RAGRetriever(vector_store_manager=db_manager)\n",
    "\n",
    "# Retrieve relevant documents\n",
    "query = \"Explain deep learning\"\n",
    "retrieved_docs = retriever.retrieve_documents(query, top_k=3)\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"\\nRetrieved {len(retrieved_docs)} documents:\")\n",
    "for i, doc in enumerate(retrieved_docs, 1):\n",
    "    print(f\"\\n{i}. Score: {doc['score']:.3f}\")\n",
    "    print(f\"   Text: {doc['text'][:100]}...\")\n",
    "print(\"‚úì RAG retrieval works\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121a2718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM Reader (this will load GPT-2 model)\n",
    "print(\"Loading LLM model (this may take a moment)...\")\n",
    "reader = LLMReader()\n",
    "\n",
    "# Combine retrieved context\n",
    "context = \"\\n\".join([doc['text'] for doc in retrieved_docs])\n",
    "\n",
    "# Generate answer\n",
    "answer = reader.generate_answer(query, context, max_length=100)\n",
    "\n",
    "print(f\"\\nQuery: {query}\")\n",
    "print(f\"\\nContext ({len(context)} chars):\")\n",
    "print(context)\n",
    "print(f\"\\nGenerated Answer:\")\n",
    "print(answer)\n",
    "print(\"\\n‚úì RAG generation works\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7eb093",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ API Testing\n",
    "\n",
    "Now let's test the FastAPI endpoints. Make sure the API is running first!\n",
    "\n",
    "```bash\n",
    "# In a terminal, run:\n",
    "python -m uvicorn api.main:app --reload --host 0.0.0.0 --port 8000\n",
    "```\n",
    "\n",
    "Or use Docker:\n",
    "```bash\n",
    "docker-compose up -d\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05417c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test API health check\n",
    "BASE_URL = \"http://localhost:8000\"\n",
    "\n",
    "try:\n",
    "    response = requests.get(f\"{BASE_URL}/health\", timeout=5)\n",
    "    if response.status_code == 200:\n",
    "        print(\"‚úì API is running!\")\n",
    "        print(f\"Response: {response.json()}\")\n",
    "    else:\n",
    "        print(f\"‚ö† API returned status {response.status_code}\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(\"‚ùå API is not running. Please start it first:\")\n",
    "    print(\"   python -m uvicorn api.main:app --reload\")\n",
    "    print(f\"   Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfc3545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test keyword extraction endpoint\n",
    "text = \"Machine learning and artificial intelligence are transforming industries worldwide.\"\n",
    "params = {\"text\": text, \"top_n\": 5}\n",
    "\n",
    "try:\n",
    "    response = requests.post(f\"{BASE_URL}/extract-keywords\", params=params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        print(\"‚úì Keyword extraction API works!\")\n",
    "        print(f\"\\nKeywords:\")\n",
    "        for keyword, score in data['keywords']:\n",
    "            print(f\"  - {keyword}: {score:.3f}\")\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2482f3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test summarization endpoint\n",
    "text = \"\"\"\n",
    "Machine learning is a method of data analysis that automates analytical model building.\n",
    "It is a branch of artificial intelligence based on the idea that systems can learn from data.\n",
    "The algorithms make predictions or decisions without being explicitly programmed to do so.\n",
    "Machine learning algorithms are used in a wide variety of applications like email filtering.\n",
    "\"\"\"\n",
    "\n",
    "params = {\n",
    "    \"text\": text,\n",
    "    \"summary_type\": \"extractive\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = requests.post(f\"{BASE_URL}/summarize\", params=params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        print(\"‚úì Summarization API works!\")\n",
    "        print(f\"\\nOriginal: {len(text)} chars\")\n",
    "        print(f\"Summary: {len(data['summary'])} chars\")\n",
    "        print(f\"Type: {data['summary_type']}\")\n",
    "        print(f\"\\nSummary:\\n{data['summary']}\")\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfdd2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test chat endpoint (Q&A)\n",
    "params = {\n",
    "    \"query\": \"What is machine learning?\",\n",
    "    \"top_k\": 3\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = requests.post(f\"{BASE_URL}/chat\", params=params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        print(\"‚úì Chat API works!\")\n",
    "        print(f\"\\nQuery: {params['query']}\")\n",
    "        print(f\"\\nAnswer:\\n{data['answer']}\")\n",
    "        print(f\"\\nSources ({len(data['sources'])}):\")\n",
    "        for i, source in enumerate(data['sources'][:3], 1):\n",
    "            print(f\"\\n{i}. {source['text'][:100]}...\")\n",
    "            print(f\"   Metadata: {source['metadata']}\")\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73280acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test documents listing endpoint\n",
    "try:\n",
    "    response = requests.get(f\"{BASE_URL}/documents\")\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        print(\"‚úì Documents API works!\")\n",
    "        print(f\"\\nTotal Documents: {data['total_documents']}\")\n",
    "        print(f\"\\nFirst {min(5, len(data['documents']))} documents:\")\n",
    "        for i, doc in enumerate(data['documents'][:5], 1):\n",
    "            print(f\"\\n{i}. ID: {doc['id']}\")\n",
    "            print(f\"   Metadata: {doc.get('metadata', {})}\")\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"‚ùå Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fadb3d",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Performance Profiling\n",
    "\n",
    "Let's benchmark the performance of our system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42edb263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check system resources\n",
    "from src.performance_profiler import system_resources\n",
    "\n",
    "print(\"System Resources:\")\n",
    "resources = system_resources()\n",
    "print(\"\\n‚úì System resources checked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510b122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark keyword extraction\n",
    "from src.performance_profiler import benchmark_keyword_extraction\n",
    "\n",
    "print(\"Benchmarking Keyword Extraction...\")\n",
    "results = benchmark_keyword_extraction()\n",
    "print(f\"\\n‚úì Benchmark complete: {results['time']:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fbf9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark summarization\n",
    "from src.performance_profiler import benchmark_summarization\n",
    "\n",
    "print(\"Benchmarking Summarization (this may take a moment)...\")\n",
    "results = benchmark_summarization()\n",
    "\n",
    "print(\"\\n‚úì Benchmark Results:\")\n",
    "for model, metrics in results.items():\n",
    "    print(f\"\\n{model.upper()}:\")\n",
    "    print(f\"  Time: {metrics['time']:.3f}s\")\n",
    "    print(f\"  Compression: {metrics['compression_ratio']:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ed42be",
   "metadata": {},
   "source": [
    "## üîü Evaluation Metrics\n",
    "\n",
    "Test the evaluation metrics for summarization and other tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d61cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test evaluation metrics\n",
    "from src.evaluation_metrics import EvaluationMetrics\n",
    "\n",
    "metrics = EvaluationMetrics()\n",
    "\n",
    "# Test ROUGE scores\n",
    "reference = \"Machine learning is a subset of artificial intelligence that enables computers to learn.\"\n",
    "hypothesis = \"Machine learning allows computers to learn and is part of AI.\"\n",
    "\n",
    "rouge_scores = metrics.compute_rouge(reference, hypothesis)\n",
    "\n",
    "print(\"ROUGE Scores:\")\n",
    "for metric, score in rouge_scores.items():\n",
    "    print(f\"  {metric}: {score:.3f}\")\n",
    "\n",
    "# Test BLEU score\n",
    "bleu_score = metrics.compute_bleu(reference, hypothesis)\n",
    "print(f\"\\nBLEU Score: {bleu_score:.3f}\")\n",
    "\n",
    "print(\"\\n‚úì Evaluation metrics work\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7f5a14",
   "metadata": {},
   "source": [
    "## üéØ Summary and Next Steps\n",
    "\n",
    "Congratulations! You've completed the AI Study Assistant tutorial! üéâ\n",
    "\n",
    "### What you've learned:\n",
    "- ‚úÖ Text preprocessing and keyword extraction\n",
    "- ‚úÖ Extractive summarization with TextRank\n",
    "- ‚úÖ Vector database with ChromaDB\n",
    "- ‚úÖ RAG pipeline (retrieval + generation)\n",
    "- ‚úÖ FastAPI endpoints testing\n",
    "- ‚úÖ Performance profiling\n",
    "- ‚úÖ Evaluation metrics\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Try with your own data:**\n",
    "   - Upload PDFs via `/upload` endpoint\n",
    "   - Ask questions about your documents\n",
    "\n",
    "2. **Fine-tune models:**\n",
    "   - Train T5 with LoRA for better summarization\n",
    "   - Fine-tune BERT for domain-specific NER\n",
    "\n",
    "3. **Deploy to production:**\n",
    "   - Use Docker Compose: `docker-compose up -d`\n",
    "   - Set up monitoring with MLflow\n",
    "   - Configure autoscaling\n",
    "\n",
    "4. **Optimize performance:**\n",
    "   - Enable GPU acceleration\n",
    "   - Implement caching\n",
    "   - Use quantized models\n",
    "\n",
    "5. **Build a frontend:**\n",
    "   - Create a web UI (React, Vue, etc.)\n",
    "   - Add authentication\n",
    "   - Implement file management\n",
    "\n",
    "### Resources:\n",
    "- üìö README.md - Complete documentation\n",
    "- üöÄ DEPLOYMENT_CHECKLIST.md - Deployment guide\n",
    "- üìñ API Docs: http://localhost:8000/docs\n",
    "- üß™ Run tests: `pytest tests/ -v`\n",
    "- üìä Profile: `python src/performance_profiler.py`\n",
    "\n",
    "Happy coding! üöÄ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
